{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`:\n",
    "- [x] get classes working w toy models \n",
    "- [ ] integrate `expt1_oob_models.py` into this or additional nb\n",
    "- [ ] move clf classes to module \n",
    "- [ ] move the mwe's below into smthg else (maybe just in docstrings?)\n",
    "- [ ] figger out clean way to do imports for class module (tricky) \n",
    "- [ ] reorganize + simplify evaluation interface \n",
    "- [ ] figger out clever way to avoid prepping text repeatedly \n",
    "- [ ] determine whether TypeC even needs a separate class (def needs minimal text prep)\n",
    "- [ ] improve preprocessing interface \n",
    "- [ ] add sub-expt w train on ~70% and eval on length subsets \n",
    "\n",
    "<hr><hr>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "#### Document length and classification accuracy\n",
    "<hr>\n",
    "\n",
    "###### Part 1: Define experiment \n",
    "1. define space of classifiers (and hypers) to consider \n",
    "2. import classes for managing arbitrary `sklearn` and `keras` models \n",
    "3. write model evaluation function \n",
    "4. write performance curve flattener \n",
    "\n",
    "###### Part 2: Prepare data (`TODO` -- reorganize splits)\n",
    "1. import text preprocessing functions/classes (`TODO` -- decide which + put in module)\n",
    "2. load and preprocess text (need >1 format for diff clf styles)\n",
    "3. split data into length subsets \n",
    "4. set aside 30% of data for evaluation, stratified by length and label\n",
    "\n",
    "###### Part 3: Conduct experiment \n",
    "1. for each train subset, get 5-fold crossval F1 for each clf\n",
    "2. for each fit, generate and save preds on evaluation set \n",
    "\n",
    "###### Part 4: Evaluate results\n",
    "1. plot the crossval F1 scores across subsets and clfs\n",
    "2. plot performance on validation set for each clf and subset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### Part 1: Define experiment\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Define space of classifiers \n",
    "\n",
    "We will consider the following set of classification strategies:\n",
    "\n",
    "- `Type A:` Non-NN: \n",
    "    - Multinomial Naive Bayes\n",
    "    - Support Vector Machine\n",
    "- `Type B:` Feed-forward NN classifiers: \n",
    "    - Multilayer Perceptron\n",
    "    - Convolutional NN\n",
    "- `Type C:` Recurrent NN classifiers: \n",
    "    - LSTM Network \n",
    "    - Bi-directional RNN\n",
    "\n",
    "> *Note:* `Type C` algorithms take sequence data as input (padded token sequences), whereas `Type A` and `Type B` take DTM-type structures as input (vectorized sequences).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Import wrapper class for each model type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from modules.expt1_classes import TypeA  # for `sklearn` classifiers \n",
    "from modules.expt1_classes import TypeB  # for `keras` Sequential models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Write function to evaluate performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use f1_score directly in the below  \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# utility func to convert a probability into a binary prediction \n",
    "def prob_to_binary(prob, threshold=.5, ret_type=int):\n",
    "  assert 0 <= prob <= 1\n",
    "  assert 0 <= threshold <= 1\n",
    "  return ret_type(prob > threshold)\n",
    "\n",
    "# TODO: is this necessary? confusing?? etc. \n",
    "def score_preds(clf, test_X, test_y, metric, pred_transformer=lambda x: x):\n",
    "  test_preds = [pred_transformer(y) for y in clf.predict(test_X)]\n",
    "  return metric(test_y, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4 Write function to flatten results dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### Part 2: Prepare data\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Import text preprocessors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Load and preprocess text and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Split data into length bins (defined in the `.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Set aside 30% of data for evaluation, stratified by length and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### Part 3: Conduct experiment\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.0 Check that classes work as designed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of full data: 50000x5\n",
      "first few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>length</th>\n",
       "      <th>length_bin</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>this film was just brilliant casting location ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>big hair big boobs bad music and a giant safet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subset  length  length_bin  label  \\\n",
       "0  train     217           2      1   \n",
       "1  train     188           2      0   \n",
       "\n",
       "                                                text  \n",
       "0  this film was just brilliant casting location ...  \n",
       "1  big hair big boobs bad music and a giant safet...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO [need to modify this + integrate into secs 2.2, 2.3, 2.4 above\n",
    "import pandas as pd\n",
    "\n",
    "data_file = 'data/imdb_decoded.csv'\n",
    "vocab_n = 10000\n",
    "\n",
    "\n",
    "dat = pd.read_csv(data_file)\n",
    "\n",
    "train = dat[dat.subset=='train']\n",
    "test = dat[dat.subset=='test']\n",
    "\n",
    "# TODO: encode labels for nn mods (not nec but good for exposition)\n",
    "train_text, train_labels = train.text, train.label\n",
    "test_text, test_labels = test.text, test.label\n",
    "\n",
    "\n",
    "print(f'shape of full data: {dat.shape[0]}x{dat.shape[1]}')\n",
    "print('first few rows:')\n",
    "display(dat.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TypeA example \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# specify classifier and associated hypers \n",
    "skl_clf = MultinomialNB\n",
    "skl_clf_params = {'alpha': .9, 'fit_prior': True}\n",
    "\n",
    "# preprocess text \n",
    "skl_token_params = {'ngram_range': (1,1), 'min_df': 2, 'max_features': vocab_n}\n",
    "vectorizer = CountVectorizer(**skl_token_params)\n",
    "train_dtm = vectorizer.fit_transform(train_text)\n",
    "test_dtm = vectorizer.transform(test_text)\n",
    "\n",
    "# instantiate and train classifier \n",
    "classifier = TypeA(skl_clf, **skl_clf_params)\n",
    "classifier.train(train_dtm, train_labels)\n",
    "\n",
    "# generate predictions on holdout set \n",
    "predsA = classifier.predict(test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/4\n",
      " - 3s - loss: 0.5318 - acc: 0.7705 - val_loss: 0.4058 - val_acc: 0.8645\n",
      "Epoch 2/4\n",
      " - 3s - loss: 0.3336 - acc: 0.8913 - val_loss: 0.3136 - val_acc: 0.8848\n",
      "Epoch 3/4\n",
      " - 3s - loss: 0.2538 - acc: 0.9136 - val_loss: 0.2990 - val_acc: 0.8909\n",
      "Epoch 4/4\n",
      " - 3s - loss: 0.2037 - acc: 0.9323 - val_loss: 0.2864 - val_acc: 0.8892\n"
     ]
    }
   ],
   "source": [
    "### TypeB example\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# specify classifier and associated hypers \n",
    "keras_model = models.Sequential\n",
    "layers_list = [layers.Dense, layers.Dense, layers.Dense]\n",
    "kwargs_list = [dict(units=16, activation='relu', input_shape=(vocab_n, )), \n",
    "               dict(units=16, activation='relu'), \n",
    "               dict(units=1,  activation='sigmoid')]\n",
    "compile_params = dict(optimizer='rmsprop', \n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "train_params = dict(valset_prop=.3, epochs=4, batch_size=500)\n",
    "\n",
    "\n",
    "# preprocess text \n",
    "tokenizer = Tokenizer(num_words=vocab_n)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "train_encoded = tokenizer.texts_to_sequences(train_text)\n",
    "test_encoded = tokenizer.texts_to_sequences(test_text)\n",
    "train_dtm = tokenizer.sequences_to_matrix(train_encoded, mode='count')\n",
    "test_dtm = tokenizer.sequences_to_matrix(test_encoded, mode='count')\n",
    "# print(f'dims of train input, targets: {train_dtm.shape}, {train_labels.shape}')\n",
    "# print(f'dims of test  input, targets: {test_dtm.shape}, {test_labels.shape}')\n",
    "\n",
    "\n",
    "# instantiate, compile, and train classifier \n",
    "# (make sure inputs and targets are appropriately preprocessed!!!) \n",
    "neural_net = TypeB(keras_model, layers_list, kwargs_list)\n",
    "neural_net.compile_model(**compile_params)\n",
    "neural_net.train(train_dtm, train_labels, **train_params)\n",
    "\n",
    "\n",
    "# generate predictions on holdout set \n",
    "predsB = neural_net.predict(test_dtm, pred_postprocessor=prob_to_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> TypeA test f1: 0.812\n",
      "  >> TypeB test f1: 0.884\n"
     ]
    }
   ],
   "source": [
    "### compare performance of TypeA and TypeB (not real expt)\n",
    "print('  >> TypeA test f1:', round(f1_score(test_labels, predsA), 3))\n",
    "print('  >> TypeB test f1:', round(f1_score(test_labels, predsB), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Get 5-fold crossval F1 for each clf across train subsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Generate predictions on evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Train on random 70% subset, evaluate across subsets on remaining 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### Part 4: Evaluate results\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Plot CV scores across subsets and clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Plot performance on holdout set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "<hr><hr>\n",
    "### sqrache areyaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false \n",
    "### this is 80 chars long, just fyi ##########################################\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def boosh(x, **kwargs):\n",
    "  print(f'x is {x}...')\n",
    "  return np.random.normal(**kwargs)\n",
    "\n",
    "def boosh2(f_list, kwargs_list):\n",
    "  assert len(f_list) == len(kwargs_list)\n",
    "  for f, kwargs in zip(f_list, kwargs_list):\n",
    "    val = f(**kwargs)\n",
    "    print(f'{f.__name__} applied to kwargs: {val}')\n",
    "\n",
    "\n",
    "# kw = dict(loc=1, scale=2, size=3)\n",
    "# np.random.normal(**kw)\n",
    "# boosh(3, **kw)\n",
    "\n",
    "# np.random.exponential(scale=1.0, size=5)\n",
    "# np.random.chisquare(df, size=1)\n",
    "\n",
    "\n",
    "fs = [np.random.normal, np.random.exponential, np.random.chisquare]\n",
    "kwargss = [dict(loc=1, scale=2, size=3), \n",
    "           dict(scale=2.0, size=2), \n",
    "           dict(df=2, size=1)]\n",
    "np.random.seed(6933)\n",
    "boosh2(fs, kwargss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### sketch of expt1 format \n",
    "<hr>\n",
    "\n",
    "```\n",
    "# here is some pseudocode illustrating the approach \n",
    "\n",
    "dat = dataset\n",
    "\n",
    "length_bins = [0,1,2,3,4]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# for each subset defined by length \n",
    "for bin in length_bins:\n",
    "  dat_subset = dat[length_bin==length_bin]\n",
    "  train_text, train_labels = dat.text[subset=='train'],  dat.label[subset=='train']\n",
    "  test_text,  test_labels  = dat.text[subset=='test'], dat.label[subset=='test']\n",
    "  \n",
    "  # preprocess the text, separately for A versus B(?!)\n",
    "  train_dtmA = A_style_transformer(train_text)\n",
    "  train_dtmB = B_style_transformer(train_text)\n",
    "  test_dtmA = A_style_transformer(test_text)\n",
    "  test_dtmB = B_style_transformer(test_text)\n",
    "  \n",
    "  # specify necessary hypers for each clf (maybe do elsewhere?!) \n",
    "  hypersA = {some-hypers-for-clf-A}\n",
    "  hypersB = {some-hypers-for-clf-B}\n",
    "  \n",
    "  # train classifiers \n",
    "  clfA = Classifier_TypeA(hypersA).train(train_dtmA, train_labels)\n",
    "  clfB = Classifier_TypeA(hypersB).train(train_dtmB, train_labels)\n",
    "  \n",
    "  # generate predictions \n",
    "  predsA = clfA.predict(test_dtmA)\n",
    "  predsB = clfB.predict(test_dtmB)\n",
    "  \n",
    "  # score the preds \n",
    "  results['bin'+str(bin)] = {'A_score': f1_score(test_labels, predsA), \n",
    "                             'B_score': f1_score(test_labels, predsB)}\n",
    "  \n",
    "    \n",
    "# postprocess the results dict \n",
    "results_df = dataframeify(results)\n",
    "results_df.to_csv('expt1_results.csv')\n",
    "\n",
    "# then go make nice plots in an R Markdown doc :p \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
