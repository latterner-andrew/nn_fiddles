'''
Utilities for preprocessing text, evaluating model preds, etc. 
[last update: oct17/2018]

Defines:
  - DTMize_factory(base_docs, vocab_limit=None, return_word_idx=False)
  - prob_to_binary(prob, threshold=.5, ret_type=int)


TODO, functionality to add:
  - [ ] arbitrary n-gram tokenization and DTM construction  
  - [ ] stopword removal 
  - [ ] clean_text() -- text cleaning (lowercase, remove num/punct, etc. etc.) 
  - [ ] summarize_corpus() -- check basic properties of corps e.g. top words, len
  - [ ] viz functions(?)

TODO, things to figure out etc.: 
  - [ ] figger out possible to do n-grams w keras Tokenizer?! 
  - [ ] figger out can a reggie or skl style DTM be used w/o issues?? 
  - [ ] decide if makes sense to have dtype decorators for vectorized corps
'''


from pandas import DataFrame
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, accuracy_score


metrics = [f1_score, precision_score, recall_score, accuracy_score]


def calculate_binary_clf_metrics(y_true, y_pred, metrics=metrics, digits=4):
  '''Calculate some standard performance metrics over predictions and labels
  
  By default, calculates 
  
  Params: 
    - y_true -- a sequence of binary labels (e.g. a list of numpy.array)
    - y_pred -- binary predictions (must be same length as `y_true`)
    - metrics -- a list of functions that summarize two lists of binary values
                 (defaults to precision, recall, F1 score, and simple accuracy)
    - digits -- number of digits to round results to (defaults to 4)
  
  Returns:
    - dict with metric name keys, and metrics applied to the args as values 
  '''
  format_key = lambda m: m.__name__.replace('_score', '')
  out = {format_key(metric): metric(y_true, y_pred) for metric in metrics}
  # or just {metric.__name__: metric(y_true, y_pred) for metric in metrics}
  return (out if digits is None 
          else {k: round(v, digits) for k, v in out.items()})




def docpadder_factory(base_docs, vocab_limit=None, return_word_idx=False):
  '''Create a function to pad sets of docs with a base vocabulary
  
  Supply a set of docs `base_docs` that restricts the vocabulary of 
  any padded sequences constructed from documents subsequently processed 
  with an instance generated by this factory. 
  
  Params:
    - base_docs -- set of docs to extract vocab from for subsequent calls 
    - vocab_limit -- upper limit on vocab length (defaults to None)
    - return_word_idx -- bool, return word index dict along w function
  
  Returns:
    - a function to pad/truncate docs given the base vocabulary 
    - if `return_word_idx=True`, also the word-index mapping from base docs
  
  TODO: 
    - finish this docstring (see below as a model)
    - consider combining w DTMize_factory() (???) 
  '''
  tokenizer = Tokenizer(num_words=vocab_limit)
  tokenizer.fit_on_texts(base_docs)
  
  def docs2padded(docs, out_length, padding='pre', truncating='pre'):
    docs_int_encoded = tokenizer.texts_to_sequences(docs)
    return pad_sequences(docs_int_encoded, maxlen=out_length, 
                         padding=padding, truncating=truncating)
  
  return (docs2padded if not return_word_idx 
          else (docs2padded, tokenizer.word_index))


  

def DTMize_factory(base_docs, vocab_limit=None, return_word_idx=False):
  '''Create a function to tokenize a set of docs with a base vocabulary
  
  Supply a set of docs `base_docs` that restricts the vocabulary of 
  any DTMs constructed from documents subsequently tokenized 
  with an instance generated by this factory. 
  
  Params:
    - base_docs -- set of docs to extract vocab from for subsequent calls 
    - vocab_limit -- upper limit on vocab length (defaults to None)
    - return_word_idx -- bool, return word index dict along w function
        - if True, returns:
            - 'docs2dtm' -- transform docs to DTM w vocab from `base_docs`
            - 'word_index' -- mapping from `base_docs` vocab to indices 
        - if False, returns just the a func from docs to DTMs, w params:
            - `docs` some docs to tokenize and transform 
            - `mode` DTM type, can be 'binary', 'count', 'freq', or 'tfidf'
            - `as_df` boolean, return DTM as a data frame for easier viewing 
  
  Returns:
    - a function to tranform docs to DTM if `return_word_idx=False`
    - the function and the word-index lookup dict if `return_word_idx=True`
  
  Usage example:
    ```
    docs = ['this is me toy corp', 'a corp is just a bunch of docs', 
            'a doc is just a string', 'a string is just some chars',
            'and this doc is a doc and the last doc of the docs']
    moredocs = ['this is me last corp corp', 'waow i has disjoint vocab yikes']
    
    # instantiate the closure to create a docs-to-DTM transformer
    DTMizer = DTMize_factory(docs, vocab_limit=None)
    
    # cast `docs` and `moredocs` to separate DTMs w a shared vocabulary 
    DTMizer(docs, mode='count', as_df=False)     # df output nice to view 
    DTMizer(moredocs, mode='count', as_df=True)  # but slow to compute on 
    
    # or: set `return_word_idx=True` to return the word-index mapping as well 
    DTMizer, word_index = DTMize_factory(docs, return_word_idx=True)
    DTMizer(docs, mode='count', as_df=True)
    word_index['doc'], [w for w, idx in word_index.items() if idx==3]    
    ```
  
  A note on the design of this func closure:
  
    For reference, here is a minimal example of the keras API's intended 
    workflow for tokenization/vectorization of text data (i.e. docs to DTM). 
  
    ```
    docs = ['this is me toy corp', 'a corp is just a bunch of docs', 
            'a doc is just a string', 'a string is just some chars',
            'and this doc is a doc and the last doc of the docs']
    
    # instantiate Tokenizer class (`num_words` to restrict vocab size)
    tokenizer = Tokenizer(num_words=num_words)
    
    # extract vocab and count words (makes several attrs available) 
    tokenizer.fit_on_texts(docs)
    
    # integer encode the documents 
    docs_int_encoded = tokenizer.texts_to_sequences(docs)
    
    # transform encoded docs into a DTM (default is binary)
    # `mode` can be one of "binary", "count", "tfidf", "freq"
    dtm = tokenizer.sequences_to_matrix(docs_int_encoded, mode='count')
    ```
    
    A naive wrapper around this routine would make it impossible to 
    transform subsequent sets of docs while maintaining the original vocab. 
    
    So we instead implement the wrapper as this closure, `DTMize_factory()`, 
    whose internal state holds the `Tokenizer` instance, including original 
    vocab and `Tokenizer.word_index` attribute. 
    
    This implementation enables the following workflow: 
    
    ```
    docs = ['this is me toy corp', 'a corp is just a bunch of docs', 
            'a doc is just a string', 'a string is just some chars',
            'and this doc is a doc and the last doc of the docs']
    moredocs = ['this is me last toy corp', 'waow i has disjoint vocab yikes']
    
    # instantiate the closure to bind the vocab to the DTM "generator" 
    DTMizer = DTMize_factory(docs, vocab_limit=None) 
    
    # note how corps can have separate DTMs but share a vocabulary: 
    DTMizer(docs, mode='count', as_df=True)
    DTMizer(moredocs, mode='count', as_df=True)
  ```  
  '''
  tokenizer = Tokenizer(num_words=vocab_limit)
  tokenizer.fit_on_texts(base_docs)
  
  # keras Tokenizer instances reserve the 0th vocab index, so left-pad vocab w '<>' 
  dtm_df_colnames = ['<>'] + list(tokenizer.word_index.keys())
  
  def docs2dtm(docs, mode, as_df=False):
    docs_int_encoded = tokenizer.texts_to_sequences(docs)
    dtm = tokenizer.sequences_to_matrix(docs_int_encoded, mode=mode)
    if as_df: dtm = DataFrame(dtm, columns=dtm_df_colnames)
    return dtm
  
  return (docs2dtm if not return_word_idx 
          else (docs2dtm, tokenizer.word_index))





def prob_to_binary(prob, threshold=.5, ret_type=int):
  '''Convert a probability into a binary value of user-specified type 
  
  Replace a real value in the unit interval [0,1] with a bool-y value indicating 
  whether it's above the value supplied by `threshold`. 
  
  Params:
    - `prob` -- a probability (float in range [0,1], inclusive on both sides) 
    - `threshold` -- the cutoff point above which True is returned (else False)
    - `ret_type` -- type of return value (bool, int, float, np.float, etc.)
  
  Usage example:
    ```
    probs = [0., .2, .5, .8, 1.]
    [prob_to_binary(prob, threshold=.5, ret_type=bool) for prob in probs]
    [prob_to_binary(prob, threshold=.5, ret_type=int) for prob in probs]
    ```
  '''
  assert 0 <= prob <= 1
  assert 0 <= threshold <= 1
  return ret_type(prob > threshold)

