{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fname = '../data/relevance_labeled_tweets_train.csv'\n",
    "\n",
    "fname_test = '../data/relevance_labeled_tweets_holdout.csv'\n",
    "\n",
    "dat = pd.read_csv(fname)\n",
    "dat_test = pd.read_csv(fname_test)\n",
    "dat.head(5)\n",
    "dat['relevant_binary'] = dat['relevant'].apply(lambda x: 1 if x else 0)\n",
    "dat_test['relevant_binary'] = dat_test['relevant'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 1599 (avg 15.0 words\n",
      "test size: 400 (avg 15.0 words)\n",
      "Train distribution\n",
      "1    1318\n",
      "0     281\n",
      "Name: relevant_binary, dtype: int64\n",
      "Test distribution\n",
      "1    330\n",
      "0     70\n",
      "Name: relevant_binary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train, test = dat, dat_test\n",
    "import numpy as np\n",
    "txt_train, y_train = np.array(train.text_cleaned), np.array(train.relevant_binary)\n",
    "txt_test, y_test = np.array(test.text_cleaned), np.array(test.relevant_binary)\n",
    "print(\"train size: {} (avg {} words\".format(len(txt_train), round(train.text_cleaned.str.split(\" \").str.len().mean())))\n",
    "print('test size: {} (avg {} words)'.format(len(txt_test),round(test.text_cleaned.str.split(\" \").str.len().mean())))\n",
    "\n",
    "print(\"Train distribution\")\n",
    "print(train.relevant_binary.value_counts())\n",
    "print(\"Test distribution\")\n",
    "print(test.relevant_binary.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# set text-related hyper params \n",
    "maxlen = 150\n",
    "max_features = 2000\n",
    "\n",
    "\n",
    "# instantiate Tokenizer class (`num_words` to restrict vocab size)\n",
    "# extract vocab and count words (makes several attrs available)\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(txt_train)\n",
    "\n",
    "# integer encode the docs\n",
    "x_train = tokenizer.texts_to_sequences(txt_train)\n",
    "x_test = tokenizer.texts_to_sequences(txt_test)\n",
    "\n",
    "# pad the sequences (default params `padding='pre', truncating='pre'`)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2.8451957295373664, 1: 0.6066009104704098}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight('balanced', [0,1], y_train)\n",
    "class_weights = {0: weights[0],\n",
    "                 1: weights[1]}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from talos.model.early_stopper import early_stopper\n",
    "from talos.model.normalizers import lr_normalizer\n",
    "def relevance_model(x_train, y_train, x_val, y_val, params):\n",
    "    model = Sequential()                            \n",
    "    model.add(Embedding(input_dim=2000,\n",
    "                        output_dim=params['hidden_dim']))\n",
    "    model.add(LSTM(params['hidden_dim'], dropout=params['dropout'], recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(Dense(1,\n",
    "                    activation=params['last_activation']))\n",
    "\n",
    "    model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "                  loss=params['loss'],\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    out = model.fit(x_train, y_train,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    epochs=params['epochs'],\n",
    "                    verbose=0,\n",
    "                    validation_data=[x_val, y_val],\n",
    "                    callbacks=early_stopper(params['epochs'], mode='strict'))\n",
    "    \n",
    "    return out, model\n",
    "\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.activations import sigmoid\n",
    "from keras.losses import binary_crossentropy\n",
    "#p = {'lr': [0.1, 0.2, 0.3, 0.4],\n",
    "#     'hidden_dim': [100, 200, 500],\n",
    "#     'batch_size': [128, 256],\n",
    "#     'epochs': [5, 10],\n",
    "#     'dropout': [0, 0.1, 0.2],\n",
    "#     'recurrent_dropout': [0, 0.1, 0.2],\n",
    "#     'optimizer': [Adam, Nadam],\n",
    "#     'loss': [binary_crossentropy],\n",
    "#     'last_activation': [sigmoid],\n",
    "#     'weight_regulizer':[None]}\n",
    "\n",
    "p = {'lr': [0.1, 0.2],\n",
    "     'hidden_dim': [100, 200],\n",
    "     'batch_size': [128, 256],\n",
    "     'epochs': [5],\n",
    "     'dropout': [0.1, 0.2],\n",
    "     'recurrent_dropout': [0.1, 0.2],\n",
    "     'optimizer': [Adam],\n",
    "     'loss': [binary_crossentropy],\n",
    "     'last_activation': [sigmoid],\n",
    "     'weight_regulizer':[None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [13:20<00:00, 23.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "h = ta.Scan(x=x_train, y=y_train, x_val=x_test, y_val=y_test, params=p,\n",
    "            model=relevance_model,\n",
    "            dataset_name='juul_relevance',\n",
    "            experiment_no='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "p = ta.Predict(h)\n",
    "preds_test = p.predict(x_test)\n",
    "print(preds_test)\n",
    "preds_test_bool = [bool(pred >= .5) for pred in preds_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 0.9248\n",
      "recall_score: 0.6333\n",
      "f1_score: 0.7518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# evaluate model performance on full test set with familiar metrics\n",
    "metrics = [precision_score, recall_score, f1_score]\n",
    "\n",
    "for metric in metrics:\n",
    "  print('{}: {}'.format(metric.__name__, round(metric(y_test, preds_test_bool), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set confusion matrix: \n",
      "[[ 53  17]\n",
      " [121 209]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#from plotting_util import human_readable_confusion_table\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, preds_test_bool)\n",
    "print('test set confusion matrix: ', conf_mat, sep='\\n')\n",
    "\n",
    "#human_readable_confusion_table(y_test, preds_test_bool)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
