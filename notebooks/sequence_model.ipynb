{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence models for text classification\n",
    "<hr>\n",
    "\n",
    "1. load, split, and preprocess data \n",
    "2. define network structure, set hyper-params, compile  \n",
    "3. train classifier \n",
    "4. generate predictions on holdout set \n",
    "5. evaluate performance \n",
    "6. Appendix: transfer learning with pre-trained word embeddings\n",
    "  - 6.1 load pre-trained embeddings\n",
    "  - 6.2 tokenize and matricize text data (replace tokens with their embeddings)\n",
    "  - 6.3 adapt network structure for input matrices \n",
    "  - 6.4 train classifier, generate predictions, evaluate performance \n",
    "\n",
    "\n",
    "**`TODO`**:\n",
    "- clean up all cells\n",
    "- add notes where nec\n",
    "- reintroduce plotting part at end\n",
    "\n",
    "**`TODO`** -- insert discussion about:\n",
    "- difference btwn sequence models + dtm-based models\n",
    "- feature \"learning\" versus engineering\n",
    "- notable hypers that dont appear in other situations\n",
    "- role of embeddings + decisions to make w.r.t. them in seq mods\n",
    "- decide whether to have separate rnn/gru/lstm nb's or just mention each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load, split, and preprocess data\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read in the movie reviews dataset `imdb_decoded.csv`, displaying the first few rows. Note that it has a field for train/test (`'subset'`), to make splitting the data easier. Note also that it has fields `'length'` and `'length_bin'`, which specify the length (in words) and length quartile the review belongs to (quartiles calculated across all 50k reviews). \n",
    "\n",
    "> Note: The data in `dat` is a re-formatted version of the IMDB movie reviews sentiment dataset, which can be accessed in integer-encoded format via `keras.datasets.load_imdb()`. If you don't have the file `imdb_decoded.csv`, just run the script `expt1_prep_imdb_data.py` to generate it (make sure `outfile` points to a valid filepath). \n",
    "\n",
    "<!-- \n",
    "  `dat` differs from the value of `keras.datasets.load_imdb()` only in format and encoding: the present version is a single rectangular data frame, the reviews have been re-encoded as English text, each review's length (in words) and train/test status is available, and the length quartile of each review is also available (in the `length_bin` field, which has values in `[0,1,2,3,4]`). \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>length</th>\n",
       "      <th>length_bin</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>this film was just brilliant casting location ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>big hair big boobs bad music and a giant safet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this has to be one of the worst films of the 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>549</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>the scots excel at storytelling the traditiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>worst mistake of my life br br i picked this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subset  length  length_bin  label  \\\n",
       "0  train     217           2      1   \n",
       "1  train     188           2      0   \n",
       "2  train     140           1      0   \n",
       "3  train     549           3      1   \n",
       "4  train     146           1      0   \n",
       "\n",
       "                                                text  \n",
       "0  this film was just brilliant casting location ...  \n",
       "1  big hair big boobs bad music and a giant safet...  \n",
       "2  this has to be one of the worst films of the 1...  \n",
       "3  the scots excel at storytelling the traditiona...  \n",
       "4  worst mistake of my life br br i picked this m...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fname = '../experiments/expt1/data/imdb_decoded.csv'\n",
    "\n",
    "dat = pd.read_csv(fname)\n",
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 25k train reviews and 25k test reviews. Average review length is approximately the same across train and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 25000 (avg 238 words)\n",
      "test size: 25000 (avg 230 words)\n"
     ]
    }
   ],
   "source": [
    "train, test = dat[dat.subset=='train'], dat[dat.subset=='test']\n",
    "\n",
    "txt_train, y_train = list(train.text), list(train.label)\n",
    "txt_test, y_test = list(test.text), list(test.label)\n",
    "\n",
    "print(f'train size: {len(txt_train)} (avg {round(train.length.mean())} words)')\n",
    "print(f'test size: {len(txt_test)} (avg {round(test.length.mean())} words)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tokenize the texts, restricting vocabulary to the words appearing in the train subset. To reduce the risk of learning useless patterns from rare words, vocabulary is also restricted to the `max_features` most frequent words (within the train set). \n",
    "\n",
    "Because we will be learning feature representations instead of engineering them, each review must have a uniform length. To achieve this, reviews longer than `maxlen` tokens are truncated to the first `maxlen` tokens, and reviews shorter than `maxlen` tokens are left-padded with the dummy token `TODO--WHA IS PAD CHAR??` until they are `maxlen`-many tokens long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# set text-related hyper params \n",
    "maxlen = 150\n",
    "max_features = 10000\n",
    "\n",
    "\n",
    "# instantiate Tokenizer class (`num_words` to restrict vocab size)\n",
    "# extract vocab and count words (makes several attrs available)\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(txt_train)\n",
    "\n",
    "# integer encode the docs\n",
    "x_train = tokenizer.texts_to_sequences(txt_train)\n",
    "x_test = tokenizer.texts_to_sequences(txt_test)\n",
    "\n",
    "# pad the sequences (default params `padding='pre', truncating='pre'`)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: It is sometimes also necessary to vectorize the labels in preparation for working with neural networks. But in the present case, we are doing binary classification so a single value (`0` or `1`) is fine. If we were predicting number of stars instead of positive/negative, then we would one-hot encode each label so that e.g. `'2 stars'` would be encoded as `[0, 1, 0, 0, 0]`, `'4 stars'` would be encoded as `[0, 0, 0, 1, 0]`, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define network structure, set hyper-params, compile\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the network structure and *compile* the network. \n",
    "\n",
    "Note that the structure and definition of a network makes no reference to the data, other than that the input dimension of the input layer must be compatible with the shape of the data, and that the output layer must produce a value that makes sense in the context of the labels (in the network defined here, the output layer produces a single probability, which makes sense since we are doing binary classification). \n",
    "\n",
    "\n",
    "##### Network structure\n",
    "There are infinitely many structures we could choose for a neural net-based classifier. The best structure depends on a huge number of factors, some related to the data (e.g. binary/multi-class, text or numeric input, sequence or non), some related to practical considerations (e.g. performance for real-time systems), and so on. \n",
    "\n",
    "Every neural network has an *input layer* and an *output layer*, but there can be any number of hidden layers between the input and output. Hidden layers can have varying numbers of nodes, the connections between the nodes can vary, and layers can be endowed with special properties like dropout. The interested reader is referred to [Chollet (2018)](TODO--link-to-DLwP) for more information. \n",
    "\n",
    "Here we will define a network with the following components: \n",
    "\n",
    "- **input layer** -- an embedding layer with input dimension `max_features` (one node for each vocabulary item) and output dimension `hidden_dim` (a tunable hyper-parameter)\n",
    "- **hidden layer 1** -- an LSTM layer with dropout and recurrent dropout (these values can also be tuned), input and output dimensions are both `hidden_dim` \n",
    "- **output layer** -- a one-unit output layer with activation function `out_activation` (produces probabilities) \n",
    "\n",
    "##### Compiling a network\n",
    "Whereas the layers and nodes of a network can be characterized mathematically, compiling a network is something that's only necessary because the network has been specified using a computer. \n",
    "\n",
    "Compiling the network configures it for training, by supplying information about how performance should be measured (loss, optimizer) and which algorithm should be used for optimizing the parameter weights (optimizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# training-related hypers (how much data and for how long)\n",
    "epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "# training-related hypers (objective and how to maximize it)\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = 'adam'\n",
    "\n",
    "# hypers for network layout + structure  \n",
    "hidden_dim = 100\n",
    "rec_dropout = 0.2\n",
    "lstm_dropout = 0.2\n",
    "out_activation = 'sigmoid'\n",
    "\n",
    "\n",
    "# define the network structure \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, hidden_dim))\n",
    "model.add(LSTM(hidden_dim, dropout=lstm_dropout,recurrent_dropout=rec_dropout))\n",
    "model.add(Dense(1, activation=out_activation))\n",
    "\n",
    "# compile the network \n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model objects of class `keras.models.Sequential` have a `.summary()` method, which you can call to see some basic info about the layers of your network, their shapes, and the number of trainable parameters they contribute to the overall model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train classifier \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will train the classifier in mini-batches of `batch_size`, for `epochs`-many runs through the entire dataset. Higher values for `epoch` are more likely to over-fit to the training data. \n",
    "\n",
    "Note also that when we specify `validation_split`, a portion of the data is not used for training, but rather to quantify the model's out-of-sample prediction accuracy. At each epoch, you will see the model's prediction accuracy on a (`valset_prop`\\*100)% subset of the samples in `x_train` -- none of which is seen during actual training. \n",
    "\n",
    "Setting aside a portion of the training data for evaluation *during training* is important because it gives you an idea of how well your mode will generalize to cases outside of the data it was trained on. \n",
    "\n",
    "Crucially, the data set aside for validation during training is *not* the same as the test set (here, `x_test` and `y_test`). The test set is meant to quantify performance on a tuned/optimized model, and should be used sparingly (ideally, only once \"the official model\" has been selected). Instead, assess the impact of parameter tweaks by looking at how the validation accuracy changes over epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "15000/15000 [==============================] - 79s 5ms/step - loss: 0.6015 - acc: 0.6889 - val_loss: 0.4496 - val_acc: 0.8062\n",
      "Epoch 2/5\n",
      "15000/15000 [==============================] - 64s 4ms/step - loss: 0.3558 - acc: 0.8579 - val_loss: 0.3870 - val_acc: 0.8292\n",
      "Epoch 3/5\n",
      "15000/15000 [==============================] - 84s 6ms/step - loss: 0.2758 - acc: 0.8935 - val_loss: 0.3589 - val_acc: 0.8578\n",
      "Epoch 4/5\n",
      "15000/15000 [==============================] - 74s 5ms/step - loss: 0.2408 - acc: 0.9081 - val_loss: 0.3945 - val_acc: 0.8483\n",
      "Epoch 5/5\n",
      "15000/15000 [==============================] - 75s 5ms/step - loss: 0.1981 - acc: 0.9276 - val_loss: 0.4120 - val_acc: 0.8428\n"
     ]
    }
   ],
   "source": [
    "# prop to set aside for accuracy calc after each epoch \n",
    "valset_prop = .40\n",
    "\n",
    "# train the model (returns a keras.callbacks.History object)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, validation_split=valset_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the accuracy and loss curves, to see how validation performance compares to training performance across training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from plotting_util import plot_keras_loss_accuracy_curves\n",
    "\n",
    "### TODO: spruce up the plots (cant even see w dark jlab theme...)\n",
    "plot_keras_loss_accuracy_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate predictions on holdout set \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate model predictions on the test set using the model's `.predict()` method. \n",
    "\n",
    "Even though the model was trained for binary classification, `model.predict()` will return a probability in [0, 1] when given a piece of text as input. This should be interpreted as the probability that the sample belongs to the positive class (\"positive\" in the sense of \"corresponding to the value `1`\"). \n",
    "\n",
    "Here, we simply flatten the probabilities so that values .5 and above are assigned `True` (positive sentiment), and values below .5 are assigned `False` (negative sentiment). \n",
    "\n",
    "> Note: We could just as easily have used the method `model.predict_classes()`, which would give us the binary predictions directly. But it can be informative to have the intermediate, probabilistic predictions if you want to introspect about why the model makes the predictions it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict() method returns probabilities \n",
    "# (use `model.predict_classes(x_test)` to get pre-flattened categorical preds)\n",
    "preds_test = model.predict(x_test)\n",
    "preds_test_bool = [bool(pred >= .5) for pred in preds_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate model performance \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of a machine learning model can be evaluated in many ways. Model evaluation is no different for neural networks than it is for any other class of model: we compare our model predictions on holdout data to the actual labels/ground truth on those same data points, summarizing the comparison with some metric(s) of our choosing. \n",
    "\n",
    "Keras model objects also have a `model.evaluate()` method, which will calculate a score by aggregating the value of some performance metric over mini-batches (see [this SO post for a quick summary](https://stackoverflow.com/a/44488571/6678726)). \n",
    "\n",
    "Here, we'll demonstrate use of the `model.evaluate()` method, and then move on to the more familiar and simple metrics of precision, recall, and F1-score. We'll also print a [confusion table](https://en.wikipedia.org/wiki/Confusion_matrix) to see how many data points in the test set fall into the categories of true positive, true negative, false positive, and false negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 26s 1ms/step\n",
      "test \"score\": 0.4189\n",
      "test accuracy: 0.8378\n"
     ]
    }
   ],
   "source": [
    "# note that evaluation also happens in batches, so can take a min to run \n",
    "score, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print(f'test \"score\": {round(score, 4)}')\n",
    "print(f'test accuracy: {round(accuracy, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 0.8722\n",
      "recall_score: 0.7915\n",
      "f1_score: 0.8299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# evaluate model performance on full test set with familiar metrics\n",
    "metrics = [precision_score, recall_score, f1_score]\n",
    "\n",
    "for metric in metrics:\n",
    "  print(f'{metric.__name__}: {round(metric(y_test, preds_test_bool), 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set confusion matrix: \n",
      "[[11050  1450]\n",
      " [ 2606  9894]]\n",
      "count (% of total) for each label-pred combo:\n",
      "  >> true neg:  11050 (44.2%)\n",
      "  >> false pos: 1450 (5.8%)\n",
      "  >> false neg: 2606 (10.4%)\n",
      "  >> true pos:  9894 (39.6%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from plotting_util import human_readable_confusion_table\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, preds_test_bool)\n",
    "print('test set confusion matrix: ', conf_mat, sep='\\n')\n",
    "\n",
    "human_readable_confusion_table(y_test, preds_test_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Appendix: Transfer learning with pre-trained word embeddings\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Load pre-trained embeddings\n",
    "> (use GloVe embeddings or w/e is easiest)\n",
    "\n",
    "1. imports + set params like emb_length etc. \n",
    "2. load embeddings\n",
    "3. show a couple vectors\n",
    "4. filter to just the ones in vocab (Tokenizer shd have it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Tokenize and matricize text \n",
    "> (replace tokens with their embeddings)\n",
    "\n",
    "1. generate (maxlen, emb_length) matrix for each text \n",
    "2. that's the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Adapt network structure \n",
    "> (for matrix inputs instead of vector inputs)\n",
    "\n",
    "1. figure out what input layer needs to be like\n",
    "2. make it like that (mite need to \"freeze\" weights at some pt)\n",
    "3. compile model + print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Train/predict/evaluate \n",
    "> (train classifier, generate predictions, evaluate performance)\n",
    "\n",
    "1. train model\n",
    "2. plot curves\n",
    "3. generate preds\n",
    "4. calculate .evaluate() metrics + std metrics + conf mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "### SCRAPS AND ACTIVE `TODO` ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: FIND PAD CHAR FOR NARRATIVE TXT IN SEC 1!!! \n",
    "# (look for a short review + find its prepped version)\n",
    "# [r for r in x_train if len(r.split()) < 150][:5]\n",
    "# dat.length\n",
    "# dat.text[11]\n",
    "# [r for r in x_train if len(r.split()) < 150][:5]\n",
    "# dat.length\n",
    "# dat.text[11]\n",
    "# dat\n",
    "# \n",
    "\n",
    "### TODO: EFFICIENT SHOW REVIEW(S) BASED ON SUBSTRING \n",
    "# ss = 'when i rented this'\n",
    "# [r for r in x_test if ss in x_test]\n",
    "# x_train[0]\n",
    "# \n",
    "\n",
    "### TODO: QUICK WAY TO LOOK AT THE TEXT OF A BAD PRED \n",
    "# # currently cant do this bc just have one var from `dat` to seq's \n",
    "# [*zip(preds_test_bool[:10], y_test[:10])]\n",
    "# x_test[3]\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
