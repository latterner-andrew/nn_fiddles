{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf fiddle with imdb sentiment dataset \n",
    "###### [last update: aug29/2018]\n",
    "<hr><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### useful links etc.\n",
    "\n",
    "- [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) for dataset `tf.keras.datasets.imdb`\n",
    "- [explanation](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset) of weird quasi-BoW format for input features\n",
    "  - see accepted answer for ex of restoring text; \n",
    "  - see second answer for original text. \n",
    "- examples of analyses using this dataset:\n",
    "  - ... \\[insert\\]\n",
    "  - ... \\[insert\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is 80 chars, just for reference (no vertical rule in jupyterlab?! </3)\n",
    "## ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ # ~ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. setup \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 load imdb sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imdb sentiment data \n",
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# ys: each a binary label, xs: each a review coded as int array\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# x_train[0]\n",
    "# wdict = imdb.get_word_index()\n",
    "# wdict['happy']\n",
    "\n",
    "x_train[:2]\n",
    "\n",
    "# [len(x) for x in x_train[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### define some utilities for handling text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict associating int indices w words \n",
    "wdict = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcs for translating between words and indices \n",
    "\n",
    "# get a word's numeric index \n",
    "def w_to_idx(w):\n",
    "  return wdict[w]\n",
    "\n",
    "# get an index's corresponding word \n",
    "def idx_to_w(widx):\n",
    "  return [w for w, idx in wdict.items() if idx == widx][0]\n",
    "\n",
    "# TODO: \n",
    "#   - figure out which is more efficient for idx_to_w():\n",
    "#       current; or \n",
    "#       next((w for w, idx in wdict.items() if idx==widx),None); or \n",
    "#       e.g. [w for w, idx in wdict.items() if idx in widxs]\n",
    "#   - return individual idx/word or lists of them?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. show structure of data and demo utils\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 basic data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of test and train sets \n",
    "print(f'train n: {len(y_train)}, test n: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first few train labels, unique label vals, and train freqs \n",
    "print(y_train[:10], np.unique(y_train, return_counts=True), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first train input \n",
    "# (corresponding label for x == y_train[0] == 1)\n",
    "x = x_train[0] \n",
    "\n",
    "print(x[:10],     # first ten elements \n",
    "      len(x),     # length is 218\n",
    "      type(x),    # x is a list... \n",
    "      type(x[0]), # of ints\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 example usage for utils `w_to_idx()`, and `idx_to_w()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example word indices and words \n",
    "widxs = [2289, 70691, 10092]\n",
    "words = ['woody', 'boosh', 'yikes']\n",
    "\n",
    "# get a word from idx, or vice versa \n",
    "print('idx', w_to_idx('woody'), 'is word', idx_to_w(2289))\n",
    "\n",
    "# get a list of words from list of idxs, or vice versa \n",
    "print('words for `widxs`: ', [idx_to_w(idx) for idx in widxs])\n",
    "print('idxs  for `words`: ', [w_to_idx(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate init segment of a review from idxs to words \n",
    "# [note that x inputs already just bags of lowercase words]\n",
    "print('first five word indices:', x[:5])\n",
    "print('word translations:', [idx_to_w(idx) for idx in x[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. train + eval a couple of `sklearn` classifiers\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re-encode preprocessed features to facilitate usual sklearn workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: CURRENTLY THIS IS WAYYYYYY TOO SLOW. SO EITHER\n",
    "###    - FIND A MORE EFFICIENT WAY; OR\n",
    "###    - USE LINK IN NOTES AT TOP OF NB TO FIND RAWER VERSION...\n",
    "\n",
    "\n",
    "# re-encode a doc as a string of its words (a 'quasi-document') \n",
    "def idxlist_to_quasi_doc(x):\n",
    "  return ' '.join([idx_to_w(idx) for idx in x])\n",
    "\n",
    "# since test/train are same size, can create qd-lists simultaneously\n",
    "quasi_docs_train = []\n",
    "quasi_docs_test = []\n",
    "\n",
    "for xidx in range(len(x_train)): \n",
    "  if not xidx % 10: print(f'on iter `{xidx}` of `{len(x_test)}`...')\n",
    "  quasi_docs_train.append(idxlist_to_quasi_doc(x_train[xidx]))\n",
    "  quasi_docs_test.append(idxlist_to_quasi_doc(x_test[xidx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 multinomial naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X=x_train, y=y_train)\n",
    "# preds_mnb = clf_mnb.predict(X=x_test)\n",
    "# accuracy_score(y_true=y_test, y_pred=preds_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 support vector machine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
